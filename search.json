[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website every time before you want to upload changes."
  },
  {
    "objectID": "content/01_journal/09_iv.html",
    "href": "content/01_journal/09_iv.html",
    "title": "Instrumental Variables",
    "section": "",
    "text": "# Load required libraries\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(estimatr)\nlibrary(AER)\n\nFirst Question: \n\n\n# Define the DAG\ndag &lt;- dagify(\n  spentTime ~ featureUsed,\n  spentTime ~ Unobserved,\n  featureUsed ~ Unobserved,\n  featureUsed ~ encourgement,\n  exposure = \"featureUsed\",\n  latent = \"Unobserved\",\n  outcome = \"spentTime\",\n  coords = list(x = c(Unobserved = 1, featureUsed = 0, spentTime = 2, encourgement = -1),\n                y = c(Unobserved = 1, featureUsed = 0, spentTime = 0, encourgement = 0)),\n  labels = c(\n    \"spentTime\" = \"Time Spent on the App\",\n    \"featureUsed\" = \"The new feature is used\",\n    \"encourgement\" = \"User encourgement to use feature\",\n    \"Unobserved\" = \"Unobserved variables\"\n  )\n)\n# Plot DAG\nggdag(dag, text = FALSE, use_labels = \"label\")\n\n\n\n\n\n\n\n\n\n#Load the data\ndf &lt;- readRDS('C:/Master Degree/Core Qualification_Compulsory Courses/Business/Causal_Data_Science/Data/Causal_Data_Science_Data/rand_enc.rds')\n\n\n# Be familiar with the data\n\nhead(df)\n\n\n\n  \n\n\n\nSecond Question: \n\n\n# Compute Naive Estimate\nnaive_estimate &lt;- lm(time_spent ~ used_ftr, data = df)\n\nsummary(naive_estimate)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = time_spent ~ used_ftr, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -20.4950  -3.5393   0.0158   3.5961  20.5051 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 18.86993    0.06955   271.3   &lt;2e-16 ***\n#&gt; used_ftr    10.82269    0.10888    99.4   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.351 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.497,  Adjusted R-squared:  0.497 \n#&gt; F-statistic:  9881 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n\nThird Question: \n\n\n# Check the correlation matrix\ncor(df) %&gt;% round(2)\n\n#&gt;            rand_enc used_ftr time_spent\n#&gt; rand_enc       1.00     0.20       0.13\n#&gt; used_ftr       0.20     1.00       0.71\n#&gt; time_spent     0.13     0.71       1.00\n\n\n\n\n\n\n\n\n\nSince the naive estimate (10.82269) is greater than the IV robust estimate using rand_enc as an instrument (9.738175), we would consider the naive estimate to have an upward bias.\nThis implies that the naive estimate overestimates the effect of used_ftr on time_spent.\nA strong correlation between used_ftr and time_spent can be seen.\nAssuming rand_enc as an Instrumental variable would make sense, because it has a weak correlation with the outcome (time_spent) and a stronger one with the treatment (used_ftr).\nWhile the correlation between Instrumental variable and outcome is not zero (maybe due to noise), this correlation is relatively low.\n\n\n\n\nFourth Question: \n\n\n# Instrumental Variable Estimation using 2SLS with rand_enc and robust standard errors\n\nmodel_iv_robust &lt;- iv_robust(time_spent ~ used_ftr | rand_enc, data = df)\nsummary(model_iv_robust)\n\n#&gt; \n#&gt; Call:\n#&gt; iv_robust(formula = time_spent ~ used_ftr | rand_enc, data = df)\n#&gt; \n#&gt; Standard error type:  HC2 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper   DF\n#&gt; (Intercept)   19.312     0.2248   85.89 0.000e+00   18.872    19.75 9998\n#&gt; used_ftr       9.738     0.5353   18.19 8.716e-73    8.689    10.79 9998\n#&gt; \n#&gt; Multiple R-squared:  0.4921 ,    Adjusted R-squared:  0.492 \n#&gt; F-statistic:   331 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n## Hansen J test\n\n# Extract residuals and fitted values from the model\nresiduals_iv &lt;- residuals(model_iv_robust)\nfitted_values_iv &lt;- fitted(model_iv_robust)\n\n# Perform Hansen J test\nhansen_test_stat &lt;- sum(residuals_iv * fitted_values_iv)\np_value_hansen &lt;- 1 - pchisq(hansen_test_stat, df = 1)\n\n# Display results\ncat(\"Hansen J Test Statistic:\", hansen_test_stat, \"\\n\")\n\n#&gt; Hansen J Test Statistic: 0\n\ncat(\"P-value:\", p_value_hansen, \"\\n\")\n\n#&gt; P-value: 1\n\n\n\n\n\n\n\n\n\nHansen J test with a test statistic close to 0 and a p-value close to 1 indicates that the instrument used in the model is not violating the over-identifying restrictions.\nIn other words, the instrument is valid for the model, and there is no evidence to suggest that the instrument is endogenous or correlated with the error term.\n\n\n\n\n\ncat(\"Naive Estimate:\", coef(naive_estimate)['used_ftr'], \"\\n\")\n\n#&gt; Naive Estimate: 10.82269\n\ncat(\"IV Robust Estimate (rand_enc):\", model_iv_robust$coefficients['used_ftr'], \"\\n\")\n\n#&gt; IV Robust Estimate (rand_enc): 9.738175\n\n\n\n\n\n\n\n\n\nSince the naive estimate (10.82269) is greater than the IV robust estimate using rand_enc as an instrument (9.738175), we would consider the naive estimate to have an upward bias.\nThis implies that the naive estimate overestimates the effect of used_ftr on time_spent."
  },
  {
    "objectID": "content/01_journal/07_matching.html",
    "href": "content/01_journal/07_matching.html",
    "title": "Matching and Subclassification",
    "section": "",
    "text": "# Load required libraries\nlibrary(dagitty)\nlibrary(dplyr)\nlibrary(ggdag)\nlibrary(ggplot2)  \nlibrary(MatchIt)\n# Load the Data\ndf &lt;- readRDS(\"C:/Master Degree/Core Qualification_Compulsory Courses/Business/Causal_Data_Science/Data/Causal_Data_Science_Data/membership.rds\")\n\n# Be familiar with the data\nhead(df)\n\n\n\n  \n\n\n# Explore the data and check relationships between variables\nsummary(df)\n\n#&gt;       age             sex         pre_avg_purch         card       \n#&gt;  Min.   :16.00   Min.   :0.0000   Min.   :-14.23   Min.   :0.0000  \n#&gt;  1st Qu.:29.80   1st Qu.:0.0000   1st Qu.: 51.82   1st Qu.:0.0000  \n#&gt;  Median :38.80   Median :1.0000   Median : 70.15   Median :0.0000  \n#&gt;  Mean   :40.37   Mean   :0.5038   Mean   : 70.42   Mean   :0.4232  \n#&gt;  3rd Qu.:49.20   3rd Qu.:1.0000   3rd Qu.: 88.79   3rd Qu.:1.0000  \n#&gt;  Max.   :90.00   Max.   :1.0000   Max.   :169.42   Max.   :1.0000  \n#&gt;    avg_purch     \n#&gt;  Min.   :-28.61  \n#&gt;  1st Qu.: 54.02  \n#&gt;  Median : 76.24  \n#&gt;  Mean   : 76.61  \n#&gt;  3rd Qu.: 98.54  \n#&gt;  Max.   :192.91\n\ncor(df)\n\n#&gt;                      age          sex pre_avg_purch        card   avg_purch\n#&gt; age           1.00000000  0.012532675   0.517506430 0.105533628 0.448632638\n#&gt; sex           0.01253267  1.000000000  -0.001221386 0.008468092 0.002181853\n#&gt; pre_avg_purch 0.51750643 -0.001221386   1.000000000 0.192333327 0.855828507\n#&gt; card          0.10553363  0.008468092   0.192333327 1.000000000 0.382352233\n#&gt; avg_purch     0.44863264  0.002181853   0.855828507 0.382352233 1.000000000"
  },
  {
    "objectID": "content/01_journal/07_matching.html#nearest-neighbor-matching",
    "href": "content/01_journal/07_matching.html#nearest-neighbor-matching",
    "title": "Matching and Subclassification",
    "section": "1 Nearest-Neighbor matching",
    "text": "1 Nearest-Neighbor matching\n\ncem &lt;- matchit(card ~ age + pre_avg_purch+sex,\n               data = df, \n               method = 'cem', \n               estimand = 'ATE')\n\n# Covariate balance\nsummary(cem)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + pre_avg_purch + sex, data = df, \n#&gt;     method = \"cem\", estimand = \"ATE\")\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2136     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3962     1.0276    0.1092\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; sex             0.0086\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 40.1743       40.1557          0.0014     0.9993    0.0016\n#&gt; pre_avg_purch       70.4611       70.0938          0.0141     0.9929    0.0044\n#&gt; sex                  0.5040        0.5040          0.0000          .    0.0000\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0064          0.1222\n#&gt; pre_avg_purch   0.0130          0.1558\n#&gt; sex             0.0000          0.0000\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.      4232\n#&gt; Matched (ESS) 5429.65    3844\n#&gt; Matched       5716.      4164\n#&gt; Unmatched       52.        68\n#&gt; Discarded        0.         0\n\n# Use matched data\n\ndf_cem &lt;- match.data(cem)\n\n# Estimation\n\nmodel_cem &lt;- lm(avg_purch ~ card, data = df_cem, weights = weights)\nsummary(model_cem)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_cem, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -159.349  -20.459   -0.151   19.863  161.528 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  69.9896     0.3984  175.66   &lt;2e-16 ***\n#&gt; card         15.2043     0.6137   24.77   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.12 on 9878 degrees of freedom\n#&gt; Multiple R-squared:  0.0585, Adjusted R-squared:  0.0584 \n#&gt; F-statistic: 613.7 on 1 and 9878 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/07_matching.html#exact-matching",
    "href": "content/01_journal/07_matching.html#exact-matching",
    "title": "Matching and Subclassification",
    "section": "2 Exact Matching",
    "text": "2 Exact Matching\n\n# replace: one-to-one or one-to-many matching\n\nnn &lt;- matchit(card ~ age + pre_avg_purch+sex,\n              data = df,\n              method = \"nearest\", # changed\n              distance = \"mahalanobis\", # changed\n              replace = T)\n\n# Covariate Balance\n\nsummary(nn)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + pre_avg_purch + sex, data = df, \n#&gt;     method = \"nearest\", distance = \"mahalanobis\", replace = T)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2064     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3936     1.0276    0.1092\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; sex             0.0086\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       41.9964          0.0026     1.0171    0.0014\n#&gt; pre_avg_purch       76.3938       76.2937          0.0038     1.0178    0.0012\n#&gt; sex                  0.5087        0.5087          0.0000          .    0.0000\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0061          0.0281\n#&gt; pre_avg_purch   0.0076          0.0301\n#&gt; sex             0.0000          0.0000\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.      4232\n#&gt; Matched (ESS) 1992.19    4232\n#&gt; Matched       2677.      4232\n#&gt; Unmatched     3091.         0\n#&gt; Discarded        0.         0\n\n# Use matched data\n\ndf_nn &lt;- match.data(nn)\n\n# Estimation\n\nmodel_nn &lt;- lm(avg_purch ~ card, data = df_nn, weights = weights)\nsummary(model_nn)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_nn, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -132.730  -21.288   -1.675   18.318  146.631 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  76.5634     0.5881  130.19   &lt;2e-16 ***\n#&gt; card         14.5957     0.7514   19.42   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.43 on 6907 degrees of freedom\n#&gt; Multiple R-squared:  0.05179,    Adjusted R-squared:  0.05166 \n#&gt; F-statistic: 377.3 on 1 and 6907 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/07_matching.html#inverse-probability-weighting",
    "href": "content/01_journal/07_matching.html#inverse-probability-weighting",
    "title": "Matching and Subclassification",
    "section": "3 Inverse Probability Weighting",
    "text": "3 Inverse Probability Weighting\n\n# Propensity scores\n\nmodel_prop &lt;- glm(card ~ age + pre_avg_purch+sex,\n                  data = df,\n                  family = binomial(link = \"logit\"))\nsummary(model_prop)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = card ~ age + pre_avg_purch + sex, family = binomial(link = \"logit\"), \n#&gt;     data = df)\n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   -1.4298676  0.0752043 -19.013   &lt;2e-16 ***\n#&gt; age            0.0011486  0.0017761   0.647    0.518    \n#&gt; pre_avg_purch  0.0148262  0.0009264  16.003   &lt;2e-16 ***\n#&gt; sex            0.0359388  0.0412622   0.871    0.384    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 13626  on 9999  degrees of freedom\n#&gt; Residual deviance: 13249  on 9996  degrees of freedom\n#&gt; AIC: 13257\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n# Add propensities to table\n\ndf_aug &lt;- df %&gt;% mutate(propensity = predict(model_prop, type = \"response\"))\n\ndf_aug\n\n\n\n  \n\n\n# Extend data by IPW scores\n\ndf_ipw &lt;- df_aug %&gt;% mutate(\n  ipw = (card/propensity) + ((1-card) / (1-propensity)))\n\ndf_ipw\n\n\n\n  \n\n\n# Look at data with IPW scores\n\ndf_ipw %&gt;% \n  select(card, age, pre_avg_purch,sex, propensity, ipw)\n\n\n\n  \n\n\n# Estimation\n\nmodel_ipw &lt;- lm(avg_purch  ~ card ,\n                data = df_ipw, \n                weights = ipw)\n\nsummary(model_ipw)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_ipw, weights = ipw)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -205.353  -28.995   -0.275   28.787  214.307 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  70.2628     0.4320  162.66   &lt;2e-16 ***\n#&gt; card         14.9573     0.6109   24.48   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 43.19 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.05657,    Adjusted R-squared:  0.05647 \n#&gt; F-statistic: 599.5 on 1 and 9998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/05_dag.html",
    "href": "content/01_journal/05_dag.html",
    "title": "Directed Acyclic Graphs",
    "section": "",
    "text": "# Load Required Libraries\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\n\nDraw DAG of Parking Spots Example \n\n\n# Draw DAG for Sales Example\nsales_dag &lt;- dagify(\n  sales ~ parking_spots,\n  sales ~ location,\n  parking_spots ~ location, # location is the confounder\n  labels = c(\n    \"sales\" = \"Sales\",\n    \"parking_spots\" = \"Parking \\n Spots\",\n    \"location\" = \"Location\"\n  )\n)\n  ggdag(sales_dag, use_labels = \"label\",text= FALSE)\n\n\n\n\n\n\n\n\nLoad the Data \n\n\n# Load the data\ncustomer_data &lt;- readRDS('C:/Master Degree/Core Qualification_Compulsory Courses/Business/Causal_Data_Science/Data/Causal_Data_Science_Data/customer_sat.rds')\n\n# Be familiar with the data\nhead(customer_data)\n\n\n\n  \n\n\n\nRegress satisfaction on follow_ups \n\n\n# Regress satisfaction on follow_ups\n  \n   ## Simple linear Regression ##\nmodel1 &lt;- lm(satisfaction ~ follow_ups, data = customer_data)\n\nsummary(model1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups, data = customer_data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -12.412  -5.257   1.733   4.506  12.588 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  78.8860     4.2717  18.467 1.04e-10 ***\n#&gt; follow_ups   -3.3093     0.6618  -5.001 0.000243 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 7.923 on 13 degrees of freedom\n#&gt; Multiple R-squared:  0.658,  Adjusted R-squared:  0.6316 \n#&gt; F-statistic: 25.01 on 1 and 13 DF,  p-value: 0.0002427\n\n\nRegress satisfaction on follow_ups and account for subscription \n\n\n# Regress satisfaction on follow_ups and account for subscription\n\n   ## Multiple Linear Regression\nmodel2 &lt;- lm(satisfaction ~ follow_ups + subscription, data = customer_data)\n\nsummary(model2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups + subscription, data = customer_data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.3222 -2.1972  0.3167  2.2667  3.9944 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           26.7667     6.6804   4.007  0.00206 ** \n#&gt; follow_ups             2.1944     0.7795   2.815  0.01682 *  \n#&gt; subscriptionPremium   44.7222     5.6213   7.956 6.88e-06 ***\n#&gt; subscriptionPremium+  18.0722     2.1659   8.344 4.37e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.958 on 11 degrees of freedom\n#&gt; Multiple R-squared:  0.9597, Adjusted R-squared:  0.9487 \n#&gt; F-statistic: 87.21 on 3 and 11 DF,  p-value: 5.956e-08\n\n\nCompare the coefficients and give explanations \n\n\n# Compare coefficients from the two models\n\ncoef_comparison &lt;- data.frame(\n  Model = c(\"Model 1\", \"Model 2\"),\n  Intercept = c(coef(model1)[1], coef(model2)[1]),\n  FollowUps = c(coef(model1)[2], coef(model2)[2]),\n  PremiumPlus = c(0, coef(model2)[grep(\"subscriptionPremium\\\\+\", names(coef(model2)))]),\n  Elite = c(0, coef(model2)[grep(\"subscriptionElite\", names(coef(model2)))])\n)\n\nprint(coef_comparison)\n\n#&gt;                        Model Intercept FollowUps PremiumPlus Elite\n#&gt;                      Model 1  78.88605 -3.309302     0.00000     0\n#&gt; subscriptionPremium+ Model 2  26.76667  2.194444    18.07222     0\n\n\n\n\n\n\n\n\n\nThe baseline of satisfaction is lower in Model 2 because it has a lower intercept compared to Model 1.\nTaking subscription levels into account changed the direction of the relationship between FollowUps and satisfaction (from negative impact into a positive one.\nThere is a positive impact on PremiumPlus, while no addition impact on the Elite subscription level.\n\n\n\n\nPlot the Data \n\n\n# Plot the data\n\n ### Simpson's Paradox : Subscription is the Confounder ###\n\n\n  ## Not conditioning on subscription\n\nsimps_not_cond &lt;- ggplot(customer_data, aes(x = follow_ups, y = satisfaction)) +\n  geom_point(alpha = 0.8) +\n  stat_smooth(method = \"lm\", se = F) +\n  labs(title = \"Relationship between Follow-ups and Satisfaction\",\n       x = \"Follow-ups\",\n       y = \"Satisfaction\")+\n  theme_minimal() +\n  theme(legend.position = \"right\") \n\n\n  ## Conditioning on subscription\n\nsimps_cond &lt;- ggplot(customer_data, aes(x = follow_ups, y = satisfaction, color = subscription)) +\n  geom_point(alpha = 0.8) +\n  stat_smooth(method = \"lm\", se = F, size = 1) +\n  labs(title = \"Relationship between Follow-ups and Satisfaction by Subscription Level\",\n       x = \"Follow-ups\",\n       y = \"Satisfaction\",\n       color = \"Subscription\") +\n  theme_minimal() +\n  theme(legend.position = \"right\") \n  \n\n\nsimps_not_cond\n\n\n\n\n\n\n\nsimps_cond"
  },
  {
    "objectID": "content/01_journal/03_regression.html",
    "href": "content/01_journal/03_regression.html",
    "title": "Regression and Statistical Inference",
    "section": "",
    "text": "# Load required libraries\nlibrary(dplyr)\nlibrary(modelr)\n\n\nLoad the Data and Check the Dimensions\n\n\n\n\n\n\n::: {.cell hash=‘03_regression_cache/html/unnamed-chunk-2_4d8836eb07dec817e10053d53133ab34’}\n\n\n\n\n::: {.cell hash=‘03_regression_cache/html/unnamed-chunk-3_d9aa80a203159189aa3778c6f16f7ba1’}\n\n\n{.r .cell-code} ## Check what data type the data has col_names &lt;- lapply(df, colnames) first_element &lt;- head(df$carwidth, 1) cat(\"carwidth's type is \", typeof(first_element), \"\\n\")\n\n\n::: {.cell-output .cell-output-stdout}\n\n\n{.r .cell-code} first_element &lt;- head(df$carbody, 1) cat(\"carbody's type is \", typeof(first_element), \"\\n\")\n\n\n::: {.cell-output .cell-output-stdout}\n\n\n{.r .cell-code} cat(\"Two types in the data: character and double\")\n\n\n::: {.cell-output .cell-output-stdout}\n\n\n#### Choose one Regressor (horsepower)\n\n\n\n\n# Display the structure of the data frame\ndf[] &lt;- lapply(df, as.numeric)\ndf %&gt;%\n  # Calculate the correlation matrix\n  cor() %&gt;% \n  # Round the correlation values to 2 decimal places\n  round(2) %&gt;% \n  # Extract the lower triangular part of the matrix\n  Matrix::tril()\n\n#&gt; 22 x 22 Matrix of class \"dtrMatrix\"\n#&gt;                  aspiration doornumber carbody drivewheel enginelocation\n#&gt; aspiration             1.00          .       .          .              .\n#&gt; doornumber               NA       1.00       .          .              .\n#&gt; carbody                  NA         NA    1.00          .              .\n#&gt; drivewheel               NA         NA      NA       1.00              .\n#&gt; enginelocation           NA         NA      NA         NA           1.00\n#&gt; wheelbase                NA         NA      NA         NA             NA\n#&gt; carlength                NA         NA      NA         NA             NA\n#&gt; carwidth                 NA         NA      NA         NA             NA\n#&gt; carheight                NA         NA      NA         NA             NA\n#&gt; curbweight               NA         NA      NA         NA             NA\n#&gt; enginetype               NA         NA      NA         NA             NA\n#&gt; cylindernumber           NA         NA      NA         NA             NA\n#&gt; enginesize               NA         NA      NA         NA             NA\n#&gt; fuelsystem               NA         NA      NA         NA             NA\n#&gt; boreratio                NA         NA      NA         NA             NA\n#&gt; stroke                   NA         NA      NA         NA             NA\n#&gt; compressionratio         NA         NA      NA         NA             NA\n#&gt; horsepower               NA         NA      NA         NA             NA\n#&gt; peakrpm                  NA         NA      NA         NA             NA\n#&gt; citympg                  NA         NA      NA         NA             NA\n#&gt; highwaympg               NA         NA      NA         NA             NA\n#&gt; price                    NA         NA      NA         NA             NA\n#&gt;                  wheelbase carlength carwidth carheight curbweight enginetype\n#&gt; aspiration               .         .        .         .          .          .\n#&gt; doornumber               .         .        .         .          .          .\n#&gt; carbody                  .         .        .         .          .          .\n#&gt; drivewheel               .         .        .         .          .          .\n#&gt; enginelocation           .         .        .         .          .          .\n#&gt; wheelbase             1.00         .        .         .          .          .\n#&gt; carlength             0.86      1.00        .         .          .          .\n#&gt; carwidth              0.77      0.83     1.00         .          .          .\n#&gt; carheight             0.54      0.44     0.20      1.00          .          .\n#&gt; curbweight            0.74      0.87     0.85      0.21       1.00          .\n#&gt; enginetype              NA        NA       NA        NA         NA       1.00\n#&gt; cylindernumber          NA        NA       NA        NA         NA         NA\n#&gt; enginesize            0.55      0.68     0.74     -0.02       0.87         NA\n#&gt; fuelsystem              NA        NA       NA        NA         NA         NA\n#&gt; boreratio             0.46      0.60     0.55      0.14       0.64         NA\n#&gt; stroke                0.07      0.07     0.11     -0.15       0.10         NA\n#&gt; compressionratio     -0.26     -0.25    -0.25     -0.05      -0.31         NA\n#&gt; horsepower            0.40      0.60     0.70     -0.09       0.82         NA\n#&gt; peakrpm              -0.22     -0.19    -0.11     -0.15      -0.16         NA\n#&gt; citympg              -0.58     -0.78    -0.74     -0.15      -0.87         NA\n#&gt; highwaympg           -0.63     -0.79    -0.75     -0.18      -0.89         NA\n#&gt; price                 0.56      0.67     0.74      0.07       0.83         NA\n#&gt;                  cylindernumber enginesize fuelsystem boreratio stroke\n#&gt; aspiration                    .          .          .         .      .\n#&gt; doornumber                    .          .          .         .      .\n#&gt; carbody                       .          .          .         .      .\n#&gt; drivewheel                    .          .          .         .      .\n#&gt; enginelocation                .          .          .         .      .\n#&gt; wheelbase                     .          .          .         .      .\n#&gt; carlength                     .          .          .         .      .\n#&gt; carwidth                      .          .          .         .      .\n#&gt; carheight                     .          .          .         .      .\n#&gt; curbweight                    .          .          .         .      .\n#&gt; enginetype                    .          .          .         .      .\n#&gt; cylindernumber             1.00          .          .         .      .\n#&gt; enginesize                   NA       1.00          .         .      .\n#&gt; fuelsystem                   NA         NA       1.00         .      .\n#&gt; boreratio                    NA       0.58         NA      1.00      .\n#&gt; stroke                       NA       0.18         NA     -0.10   1.00\n#&gt; compressionratio             NA      -0.16         NA     -0.20  -0.30\n#&gt; horsepower                   NA       0.85         NA      0.59   0.11\n#&gt; peakrpm                      NA      -0.18         NA     -0.24   0.07\n#&gt; citympg                      NA      -0.74         NA     -0.62  -0.09\n#&gt; highwaympg                   NA      -0.76         NA     -0.61  -0.07\n#&gt; price                        NA       0.89         NA      0.55   0.03\n#&gt;                  compressionratio horsepower peakrpm citympg highwaympg price\n#&gt; aspiration                      .          .       .       .          .     .\n#&gt; doornumber                      .          .       .       .          .     .\n#&gt; carbody                         .          .       .       .          .     .\n#&gt; drivewheel                      .          .       .       .          .     .\n#&gt; enginelocation                  .          .       .       .          .     .\n#&gt; wheelbase                       .          .       .       .          .     .\n#&gt; carlength                       .          .       .       .          .     .\n#&gt; carwidth                        .          .       .       .          .     .\n#&gt; carheight                       .          .       .       .          .     .\n#&gt; curbweight                      .          .       .       .          .     .\n#&gt; enginetype                      .          .       .       .          .     .\n#&gt; cylindernumber                  .          .       .       .          .     .\n#&gt; enginesize                      .          .       .       .          .     .\n#&gt; fuelsystem                      .          .       .       .          .     .\n#&gt; boreratio                       .          .       .       .          .     .\n#&gt; stroke                          .          .       .       .          .     .\n#&gt; compressionratio             1.00          .       .       .          .     .\n#&gt; horsepower                  -0.22       1.00       .       .          .     .\n#&gt; peakrpm                      0.16       0.08    1.00       .          .     .\n#&gt; citympg                      0.44      -0.81    0.03    1.00          .     .\n#&gt; highwaympg                   0.45      -0.78    0.05    0.98       1.00     .\n#&gt; price                       -0.18       0.84   -0.02   -0.74      -0.74  1.00\n\n  # Explain what data type it is and what values it can take on\n\nfirst_element &lt;- head(df$horsepower, 1)\nmin_value &lt;- min(df$horsepower)\nmax_value &lt;- max(df$horsepower)\ncat(\"The chosen predictor is 'horsepower' with a data type of \", typeof(first_element), \" and a range between \", min_value, \" and \", max_value, \"\\n\")\n\n#&gt; The chosen predictor is 'horsepower' with a data type of  double  and a range between  48  and  288\n\ncat(\"It significantly influences the price with a correlation of 84%. The positive correlation indicates that as horsepower increases, the price also increases proportionally.\\n\")\n\n#&gt; It significantly influences the price with a correlation of 84%. The positive correlation indicates that as horsepower increases, the price also increases proportionally.\n\n\n\n\nAdd seat_heating variable to the data\n\n\nseat_heating_add_to_df &lt;- df %&gt;% mutate(seat_heating_all_true = sample(c(TRUE, FALSE), size = nrow(df), replace = TRUE))\n\n# Check the distribution of seat_heating_all_true\ntable(seat_heating_add_to_df$seat_heating_all_true)\n\n#&gt; \n#&gt; FALSE  TRUE \n#&gt;    87    94\n\n# Run the regression \nmodel &lt;- lm(price ~ seat_heating_all_true, data = seat_heating_add_to_df)\n\n# Print the summary of the model\nsummary(model)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ seat_heating_all_true, data = seat_heating_add_to_df)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -8398  -5256  -2771   2949  33091 \n#&gt; \n#&gt; Coefficients:\n#&gt;                           Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                  13746        864  15.910   &lt;2e-16 ***\n#&gt; seat_heating_all_trueTRUE    -1437       1199  -1.199    0.232    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 8058 on 179 degrees of freedom\n#&gt; Multiple R-squared:  0.007962,   Adjusted R-squared:  0.00242 \n#&gt; F-statistic: 1.437 on 1 and 179 DF,  p-value: 0.2323\n\n\n\n\n\n\n\n\n\nAssigning all values for seat_heating as true may result in no variation. This can cause challenges in regression analysis, such as collinearity and difficulty estimating coefficients.."
  },
  {
    "objectID": "content/01_journal/01_probability.html",
    "href": "content/01_journal/01_probability.html",
    "title": "Probability Theory",
    "section": "",
    "text": "#|echo: false\n\n\n# Import the required Libraries \nlibrary(tidyverse)\nlibrary(ggVennDiagram)\n\n\nAssignment I\n\n\n# Define probabilities\nProb_S &lt;- 0.3 # change in scope\nProb_T_S &lt;- 0.2 # change in scope-on time\nProb_T_not_S &lt;- 0.6 # no change in scope-on time\n\n# Calculate the complement of probabilities\nProb_not_S &lt;- 1 - Prob_S  # no change in scope\nProb_not_T_S &lt;- 1 - Prob_T_S # change in scope_not on time\nProb_not_T_not_S &lt;- 1 - Prob_T_not_S # no change in scope_not on time\n\n# Calculate conditional probabilities\nProb_T_and_S &lt;- Prob_S * Prob_T_S\nProb_T_and_not_S &lt;- Prob_not_S * Prob_T_not_S\nProb_not_T_and_S &lt;- Prob_S * Prob_not_T_S\nProb_not_T_and_not_S &lt;- Prob_not_S * Prob_not_T_not_S\n\n# Print Results\nprint(Prob_T_and_S)\n\n#&gt; [1] 0.06\n\nprint(Prob_T_and_not_S)\n\n#&gt; [1] 0.42\n\nprint(Prob_not_T_and_S)\n\n#&gt; [1] 0.24\n\nprint(Prob_not_T_and_not_S)\n\n#&gt; [1] 0.28\n\n\n\n\nAssignment II\n\n\n# Define the number of observations\nnum_observations &lt;- 1000\n\n# Generate a tibble for user device data\nuser_device_data &lt;- tibble(\n  # Assign user IDs in ascending order\n  user_id = 1:num_observations,\n  # Randomly determine smartphone usage\n  smartphone = rbinom(num_observations, 1, 0.4),\n  # Decide tablet usage, more likely if smartphone is not used\n  tablet = ifelse(smartphone == 1, rbinom(num_observations, 1, 0.2), rbinom(num_observations, 1, 0.5)),\n  # Decide computer usage, more likely if tablet is not used\n  computer = ifelse(tablet == 1, rbinom(num_observations, 1, 0.1), rbinom(num_observations, 1, 0.3))\n)\n\n# If none of the devices have a value of 1, set smartphone to 1\nuser_device_data &lt;- user_device_data %&gt;%\n  rowwise() %&gt;% \n  mutate(smartphone = ifelse(sum(smartphone, tablet, computer) == 0, 1, smartphone))\n\n# Display the first ten rows\nhead(user_device_data, 10)\n\n\n\n  \n\n\n# Display column sums\ncolSums(user_device_data)\n\n#&gt;    user_id smartphone     tablet   computer \n#&gt;     500500        600        397        224\n\n# Create sets of smartphone, tablet, and computer users\nset_smartphone &lt;- which(user_device_data$smartphone == 1)\nset_tablet &lt;- which(user_device_data$tablet == 1)\nset_computer &lt;- which(user_device_data$computer == 1)\n\n# Combine all sets into a list\nsets_all_devices &lt;- list(set_smartphone, set_tablet, set_computer)\n\n# Generate a Venn diagram\nggVennDiagram(sets_all_devices, category.names = c(\"Smartphone\", \"Tablet\", \"Computer\"),\n              label_percent_digit = 2) +\n  # Customize appearance\n  theme(legend.position = \"none\", \n        panel.background = element_rect(\"grey\"),\n        strip.background = element_rect(\"grey\")) +\n  scale_x_continuous(expand = expansion(mult = .24))\n\n\n\n\n\n\n\n# Calculate the intersection of sets (users using all three devices)\nset_all_three_devices &lt;- Reduce(intersect, sets_all_devices)\npercentage_all_three_devices &lt;- length(set_all_three_devices) / num_observations * 100\ncat(\"Percentage of customers using all three devices:\", round(percentage_all_three_devices, 2), \"%\\n\")\n\n#&gt; Percentage of customers using all three devices: 1.2 %\n\n# Calculate the union of sets (users using at least two devices)\nset_at_least_two_devices &lt;- union(intersect(set_smartphone, set_tablet), union(intersect(set_smartphone, set_computer), intersect(set_computer, set_tablet)))\npercentage_at_least_two_devices &lt;- length(set_at_least_two_devices) / num_observations * 100\ncat(\"Percentage of customers using at least two devices:\", round(percentage_at_least_two_devices, 2), \"%\\n\")\n\n#&gt; Percentage of customers using at least two devices: 20.9 %\n\n# Calculate the sets of users using each device exclusively\nset_only_smartphone &lt;- set_smartphone[!(set_smartphone %in% set_tablet) & !(set_smartphone %in% set_computer)]\nset_only_tablet &lt;- set_tablet[!(set_tablet %in% set_smartphone) & !(set_tablet %in% set_computer)]\nset_only_computer &lt;- set_computer[!(set_computer %in% set_smartphone) & !(set_computer %in% set_tablet)]\npercentage_only_one_device &lt;- (length(set_only_smartphone) + length(set_only_tablet) + length(set_only_computer)) / num_observations * 100\ncat(\"Percentage of customers using only one device:\", round(percentage_only_one_device, 2), \"%\\n\")\n\n#&gt; Percentage of customers using only one device: 79.1 %\n\n\n\n\nAssignment III\n\n\n# Probability of A\nProb_A &lt;- 0.04 \n\n# Probability of B given A\nProb_B_A &lt;- 0.97 \n\n# Probability of B given not A \nProb_B_not_A &lt;- 0.01\n\n# Probability of not A\nProb_not_A &lt;- 1- Prob_A \n\n# Probability of not B given A\nProb_not_B_A &lt;- 1 - Prob_B_A \n\n# Probability of not B given not A\nProb_not_B_not_A &lt;- 1 - Prob_B_not_A \n\n# Probability of B\nProb_B &lt;- (Prob_B_A*Prob_A)+(Prob_B_not_A*Prob_not_A)\n\n# Probability of not A given B\nProb_not_A_given_B &lt;- (Prob_B_not_A * Prob_not_A)/Prob_B\n\n# Probability of A given B\nProb_A_given_B &lt;- (Prob_B_A * Prob_A)/Prob_B\n\n# Print Results\nprint(Prob_not_A_given_B)\n\n#&gt; [1] 0.1983471\n\nprint(Prob_A_given_B)\n\n#&gt; [1] 0.8016529"
  },
  {
    "objectID": "content/01_journal/02_statistics.html",
    "href": "content/01_journal/02_statistics.html",
    "title": "Statistical Concepts",
    "section": "",
    "text": "Load the Data \n\n\n# Load the data\ndf &lt;- readRDS(\"C:/Master Degree/Core Qualification_Compulsory Courses/Business/Causal_Data_Science/Data/Causal_Data_Science_Data/random_vars.rds\")\n\n# Be familiar with the data\nhead(df)\n\n\n\n  \n\n\n\nGet an Overview of the Data \n\n\nView(df)\n\n\n# Store columns in variables\nage &lt;- df$age\nincome &lt;- df$income\n\n# 1) \n\na) Calculate the Expected Value \n\n\nexpected_age &lt;- sum(age) / length(age)\nexpected_income &lt;- sum(income) / length(income)\n\ncat(\"Expected age is \", expected_age, \"\\n\")\n\n#&gt; Expected age is  33.471\n\ncat(\"Expected income is \", expected_income, \"\\n\")\n\n#&gt; Expected income is  3510.731\n\n\nb) Calculate the Variance \n\n\nmean_age &lt;- mean(age)\nmean_income &lt;- mean(income)\n\nvariance_age &lt;- sum((age - mean_age)^2) / (length(age) - 1)\nvariance_income &lt;- sum((income - mean_income)^2) / (length(income) - 1)\n\ncat(\"Age Variance is \", variance_age, \"\\n\")\n\n#&gt; Age Variance is  340.6078\n\ncat(\"Income Variance is \", variance_income, \"\\n\")\n\n#&gt; Income Variance is  8625646\n\n\nc) Calculate the Standard Deviation \n\n\nSD_age &lt;- sqrt(variance_age)\nSD_income &lt;- sqrt(variance_income)\ncat(\"SD of Age is \", SD_age, \"\\n\")\n\n#&gt; SD of Age is  18.45556\n\ncat(\"SD of Income is \", SD_income, \"\\n\")\n\n#&gt; SD of Income is  2936.945\n\n\n2) Standard Deviation Comparison\n\n\n\n\n\n\n\n\nWhen the variables are measured in the same units and have similar scales, comparing standard deviations directly is most useful.\nIn the instance of age and income, the variables have distinct units and scales. As a result, directly comparing their standard deviations may not yield relevant insights.\n\n\n\n\n3) The Relationship Between Covariance and Correlation\n\n\ncovariance_value &lt;- sum((age - mean_age) * (income - mean_income)) /length(age)\ncat(\"Covariance Value:\", covariance_value, \"\\n\")\n\n#&gt; Covariance Value: 29670.45\n\ncorrelation_value &lt;- covariance_value / (SD_age * SD_income)\ncat(\"Correlation Value:\", correlation_value, \"\\n\")\n\n#&gt; Correlation Value: 0.5473952\n\n\n4) Interpretation regarding the Relationship Between Covariance and Correlation\n\n\n\n\n\n\n\n\nThe standardized scale of the correlation coefficient makes comparison easier. The fact that it goes from -1 to 1 gives for a clear grasp of the relationship’s strength and direction.\n\n\n\n\n5) Compute the Conditional Expected Value for:\n\n\n # 1. E[income|age&lt;=18]\n\nsubset_1 &lt;-  df %&gt;% filter(age &lt;= 18)\nconditional_expected_value1 &lt;- mean(subset_1$income)\ncat(\"Conditional Expected Value of Income for age &lt;= 18:\", conditional_expected_value1, \"\\n\")\n\n#&gt; Conditional Expected Value of Income for age &lt;= 18: 389.6074\n\n# 2. E[income|age&lt;=[18,65)]\n\nsubset_2 &lt;- df %&gt;% filter(age &gt;= 18 & age &lt; 65)\nconditional_expected_value2 &lt;- mean(subset_2$income)\ncat(\"Conditional Expected Value of Income for age in [18, 65):\", conditional_expected_value2, \"\\n\")\n\n#&gt; Conditional Expected Value of Income for age in [18, 65): 4685.734\n\n#3. E[income|age&gt;=65]\n\nsubset_3 &lt;- df %&gt;% filter(age &gt;= 65)\nconditional_expected_value3 &lt;- mean(subset_3$income)\ncat(\"Conditional Expected Value of Income for age &gt;= 65:\", conditional_expected_value3, \"\\n\")\n\n#&gt; Conditional Expected Value of Income for age &gt;= 65: 1777.237"
  },
  {
    "objectID": "content/01_journal/04_causality.html",
    "href": "content/01_journal/04_causality.html",
    "title": "Causality",
    "section": "",
    "text": "# Loading the ggplot2 library\nlibrary(ggplot2)\n\n\n# Simulating a dataset for the correlation between Facebook friends and academic performance with a potential confounding variable (study hours)\n\n# Generate sample data for a few students\nset.seed(123)\nstudents &lt;- c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\")\nfacebook_friends &lt;- rpois(length(students), lambda = 300)\nacademic_performance &lt;- rnorm(length(students), mean = 75, sd = 10)\nstudy_hours &lt;- rnorm(length(students), mean = 20, sd = 5)\n\n# Creating a dataframe\ndata &lt;- data.frame(Student = students, FacebookFriends = facebook_friends, AcademicPerformance = academic_performance, StudyHours = study_hours)\n\n\n\n# Creating a scatter plot with color representing study hours\nggplot(data, aes(x = FacebookFriends, y = AcademicPerformance, color = StudyHours)) +\n  geom_point() +\n  labs(title = \"Spurious Correlation: Facebook Friends vs Academic Performance\",\n       x = \"Facebook Friends\",\n       y = \"Academic Performance\",\n       color = \"Study Hours\") +\n  theme_minimal()"
  },
  {
    "objectID": "content/01_journal/06_rct.html",
    "href": "content/01_journal/06_rct.html",
    "title": "Randomized Controlled Trials",
    "section": "",
    "text": "# Load required libraries\nlibrary(ggplot2)\n\n\n# Load the Data\ndf &lt;- readRDS(\"C:/Master Degree/Core Qualification_Compulsory Courses/Business/Causal_Data_Science/Data/Causal_Data_Science_Data//abtest_online.rds\")\n\n# Be familiar with the data\nhead(df)\n\n\n\n  \n\n\n\n\nPlot independent and and dependent difference\n\n\n# purchase_amount (dependent)\n\npurchase_amount_comp &lt;- \n  ggplot(df, \n         aes(x = chatbot, \n             y = purchase_amount, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"purchase_amount\", title = \"Difference in purchase amount\")+\n  scale_x_discrete(labels = c(\"Not Treated\",\"Treated\"))\n\n\n# previous_visit (independent)\n\nprevious_visit_comp &lt;- \n  ggplot(df, \n         aes(x = chatbot, \n             y = previous_visit, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"previous_visit\", title = \"Difference in previous visit\")+\n  scale_x_discrete(labels = c(\"Not Treated\",\"Treated\"))\n\n\n# mobile_device (independent)\nmobile_device_comp &lt;- \n  ggplot(df, \n         aes(x = chatbot, \n             y = mobile_device, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"mobile_device\", title = \"Difference in mobile device\")+\n  scale_x_discrete(labels = c(\"Not Treated\",\"Treated\"))\n\n# purchase (dependent)\n\npurchase_comp &lt;- \n  ggplot(df, \n         aes(x = chatbot,\n             y = purchase, \n             color = as.factor(chatbot))) +\n  stat_summary(geom = \"errorbar\", \n               width = .5,\n               fun.data = \"mean_se\", \n               fun.args = list(mult=1.96),\n               show.legend = F) +\n  labs(x = NULL, y = \"purchase\", title = \"Difference in purchase\")+\n  scale_x_discrete(labels = c(\"Not Treated\",\"Treated\"))\n\n# Plot the Comparisons\n\npurchase_amount_comp\n\n\n\n\n\n\n\nprevious_visit_comp\n\n\n\n\n\n\n\nmobile_device_comp\n\n\n\n\n\n\n\npurchase_comp\n\n\n\n\n\n\n\n\n\n\nLinear Regression to find the effect of chatbot on sales\n\n\nmodel1 &lt;- lm(purchase ~ chatbot, data = df)\nsummary(model1)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase ~ chatbot, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -0.4960 -0.3249 -0.2679  0.5040  0.7321 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.49597    0.02122  23.376  &lt; 2e-16 ***\n#&gt; chatbotTRUE -0.22811    0.02989  -7.633 5.36e-14 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.4725 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.05516,    Adjusted R-squared:  0.05421 \n#&gt; F-statistic: 58.26 on 1 and 998 DF,  p-value: 5.36e-14\n\nmodel2 &lt;- lm(purchase_amount ~ chatbot, data = df)\nsummary(model2)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -16.702 -14.478  -9.626  13.922  64.648 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  16.7017     0.8374  19.944  &lt; 2e-16 ***\n#&gt; chatbotTRUE  -7.0756     1.1796  -5.998 2.79e-09 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.65 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.0348, Adjusted R-squared:  0.03383 \n#&gt; F-statistic: 35.98 on 1 and 998 DF,  p-value: 2.787e-09\n\n\n\n\nCompute CATE for mobile_device\n\n\nmodel3 &lt;- lm(purchase ~ chatbot * mobile_device, data = df)\nsummary(model3)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase ~ chatbot * mobile_device, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -0.5000 -0.3284 -0.2635  0.5000  0.7484 \n#&gt; \n#&gt; Coefficients:\n#&gt;                               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                    0.50000    0.02572  19.438  &lt; 2e-16 ***\n#&gt; chatbotTRUE                   -0.22464    0.03619  -6.207 7.93e-10 ***\n#&gt; mobile_deviceTRUE             -0.01266    0.04558  -0.278    0.781    \n#&gt; chatbotTRUE:mobile_deviceTRUE -0.01113    0.06428  -0.173    0.863    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.4729 on 996 degrees of freedom\n#&gt; Multiple R-squared:  0.05549,    Adjusted R-squared:  0.05264 \n#&gt; F-statistic:  19.5 on 3 and 996 DF,  p-value: 2.714e-12\n\nmodel4 &lt;- lm(purchase_amount ~ chatbot * mobile_device, data = df)\nsummary(model4)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_amount ~ chatbot * mobile_device, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -16.98 -14.54  -9.95  14.13  65.24 \n#&gt; \n#&gt; Coefficients:\n#&gt;                               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                    16.9797     1.0152  16.725   &lt;2e-16 ***\n#&gt; chatbotTRUE                    -7.0301     1.4284  -4.922    1e-06 ***\n#&gt; mobile_deviceTRUE              -0.8727     1.7987  -0.485    0.628    \n#&gt; chatbotTRUE:mobile_deviceTRUE  -0.1526     2.5369  -0.060    0.952    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 18.66 on 996 degrees of freedom\n#&gt; Multiple R-squared:  0.03534,    Adjusted R-squared:  0.03244 \n#&gt; F-statistic: 12.16 on 3 and 996 DF,  p-value: 8.034e-08\n\n\n\n\nLogistic regression model\n\n\nlog_model &lt;- glm(purchase ~ chatbot, family = binomial(link = 'logit'), data = df)\n\nsummary(log_model)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = purchase ~ chatbot, family = binomial(link = \"logit\"), \n#&gt;     data = df)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept) -0.01613    0.08981  -0.180    0.857    \n#&gt; chatbotTRUE -0.98939    0.13484  -7.337 2.18e-13 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 1329.1  on 999  degrees of freedom\n#&gt; Residual deviance: 1273.3  on 998  degrees of freedom\n#&gt; AIC: 1277.3\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n\n\n\n\n\n\n\n\nThe chatbot_true coefficient is very intriguing. Because it has a negative estimate, it implies that the existence of the chatbot is related with a drop in the log-odds of making a purchase as compared to when the chatbot is not present.\nThe fact that the p-value is less than 0.001 implies that the effect is statistically significant. In simpler terms, the model implies that when the chatbot is there, consumers are less likely to make a purchase, and this effect is unlikely to be attributable to chance.\nModel fit statistics include the null and residual deviance as well as the AIC (Akaike Information Criterion).\nIn general, lower AIC values suggest better-fitting models. The residual deviation is 1273.3 on 998 degrees of freedom in this situation, and the AIC is 1277.3."
  },
  {
    "objectID": "content/01_journal/08_did.html",
    "href": "content/01_journal/08_did.html",
    "title": "Difference-in-Differences",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(dplyr)\nlibrary(readr)\nlibrary(lmtest)\n\n\n# Load the data\ndata &lt;- readRDS('C:/Master Degree/Core Qualification_Compulsory Courses/Business/Causal_Data_Science/Data/Causal_Data_Science_Data/hospdd.rds')\n\n# Be familiar with the data\nhead(data)\n\n\n\n  \n\n\n\n\n1. Manually compute the mean satisfaction for treated and control hospitals before and after the treatment\n\n\n# The month after that they introduced the new admission \nthreshold_month &lt;- 3.0\n\n# The hospital-ID, after which the others are not treated\nthreshold_hospital &lt;- 18\n\n# Convert Month and hospital columns into numeric\ndata$month &lt;- as.numeric(data$month)\ndata$hospital &lt;- as.numeric(data$hospital)\n\n# Split the data into treated and controlled groups\ntreated_group &lt;- data %&gt;%\n  filter(hospital &lt;= 18 )\n\ncontrol_group &lt;- data %&gt;%\n  filter(hospital &gt; 18 )\n\n\n# Mean Difference between treatment and control group BEFORE treatment\nbefore_control_mean &lt;- control_group %&gt;% \n  filter(month &lt;= threshold_month) %&gt;%\n  summarise(mean_satisfaction = mean(satis)) %&gt;%\n  pull(mean_satisfaction)\n\nbefore_treatment_mean  &lt;- treated_group %&gt;% \n  filter( month &lt;= threshold_month) %&gt;%\n  summarise(mean_satisfaction = mean(satis)) %&gt;%\n  pull(mean_satisfaction)\n\n\nmean_diff_before &lt;- before_treatment_mean - before_control_mean\n\n\n# Mean Difference between treatment and control group AFTER treatment\nafter_control_mean &lt;- control_group %&gt;% \n  filter(month &gt; threshold_month) %&gt;%\n  summarise(mean_satisfaction = mean(satis)) %&gt;%\n  pull(mean_satisfaction)\n\nafter_treatment_mean &lt;- treated_group %&gt;% \n  filter( month &gt; threshold_month) %&gt;%\n  summarise(mean_satisfaction = mean(satis)) %&gt;%\n  pull(mean_satisfaction)\n\n\nmean_diff_after &lt;- after_treatment_mean - after_control_mean\n\n\n# Difference-in-differences\n\nmean_diff_diff &lt;- mean_diff_after - mean_diff_before\n\n\n\n2. Use linear regression to compute the estimate with group and time fixed effects\n\n\n# Fit the linear regression model\nmodel &lt;- lm(satis ~ procedure * as.factor(month) + as.factor(hospital) , data)\n\n# Print the results\nsummary(model)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ procedure * as.factor(month) + as.factor(hospital), \n#&gt;     data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.1880 -0.4630  0.0024  0.4540  4.3181 \n#&gt; \n#&gt; Coefficients: (3 not defined because of singularities)\n#&gt;                               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                  3.1716566  0.0562244  56.411  &lt; 2e-16 ***\n#&gt; procedure                    0.8785070  0.0541087  16.236  &lt; 2e-16 ***\n#&gt; as.factor(month)2           -0.0096077  0.0292138  -0.329 0.742261    \n#&gt; as.factor(month)3            0.0219686  0.0292138   0.752 0.452080    \n#&gt; as.factor(month)4            0.0152441  0.0368748   0.413 0.679326    \n#&gt; as.factor(month)5           -0.0246564  0.0368748  -0.669 0.503740    \n#&gt; as.factor(month)6            0.0055796  0.0368748   0.151 0.879734    \n#&gt; as.factor(month)7           -0.0238856  0.0368748  -0.648 0.517169    \n#&gt; as.factor(hospital)2         0.4085664  0.0772468   5.289 1.26e-07 ***\n#&gt; as.factor(hospital)3         0.5336248  0.0793436   6.725 1.88e-11 ***\n#&gt; as.factor(hospital)4         0.2275102  0.0739460   3.077 0.002101 ** \n#&gt; as.factor(hospital)5        -0.1453529  0.0739460  -1.966 0.049375 *  \n#&gt; as.factor(hospital)6         0.4478634  0.0739460   6.057 1.46e-09 ***\n#&gt; as.factor(hospital)7         1.4044164  0.0714606  19.653  &lt; 2e-16 ***\n#&gt; as.factor(hospital)8         0.0718758  0.0763236   0.942 0.346365    \n#&gt; as.factor(hospital)9        -1.5185150  0.0782498 -19.406  &lt; 2e-16 ***\n#&gt; as.factor(hospital)10        1.6828446  0.0772468  21.785  &lt; 2e-16 ***\n#&gt; as.factor(hospital)11        0.2209653  0.0763236   2.895 0.003801 ** \n#&gt; as.factor(hospital)12       -0.0953034  0.0782498  -1.218 0.223287    \n#&gt; as.factor(hospital)13        0.4955931  0.0754708   6.567 5.50e-11 ***\n#&gt; as.factor(hospital)14        0.2330426  0.0793436   2.937 0.003323 ** \n#&gt; as.factor(hospital)15       -0.1444935  0.0793436  -1.821 0.068631 .  \n#&gt; as.factor(hospital)16        1.4142680  0.0772468  18.308  &lt; 2e-16 ***\n#&gt; as.factor(hospital)17        0.4235429  0.0805415   5.259 1.49e-07 ***\n#&gt; as.factor(hospital)18        0.1532761  0.0938225   1.634 0.102369    \n#&gt; as.factor(hospital)19       -0.7453017  0.0811676  -9.182  &lt; 2e-16 ***\n#&gt; as.factor(hospital)20        0.0473874  0.0791192   0.599 0.549234    \n#&gt; as.factor(hospital)21        1.1943370  0.0836287  14.281  &lt; 2e-16 ***\n#&gt; as.factor(hospital)22        0.7993153  0.0823390   9.708  &lt; 2e-16 ***\n#&gt; as.factor(hospital)23        0.7017202  0.0811676   8.645  &lt; 2e-16 ***\n#&gt; as.factor(hospital)24       -0.3081260  0.0866459  -3.556 0.000379 ***\n#&gt; as.factor(hospital)25        0.6464736  0.0927319   6.971 3.41e-12 ***\n#&gt; as.factor(hospital)26        0.2142471  0.0791192   2.708 0.006787 ** \n#&gt; as.factor(hospital)27       -0.3986544  0.0766156  -5.203 2.01e-07 ***\n#&gt; as.factor(hospital)28        0.7119953  0.0836287   8.514  &lt; 2e-16 ***\n#&gt; as.factor(hospital)29        0.2485512  0.0800987   3.103 0.001923 ** \n#&gt; as.factor(hospital)30       -0.1679220  0.0953700  -1.761 0.078324 .  \n#&gt; as.factor(hospital)31        0.5120848  0.0791192   6.472 1.03e-10 ***\n#&gt; as.factor(hospital)32       -0.3233456  0.0800987  -4.037 5.47e-05 ***\n#&gt; as.factor(hospital)33       -0.4539752  0.0791192  -5.738 9.97e-09 ***\n#&gt; as.factor(hospital)34       -0.0004123  0.0746103  -0.006 0.995591    \n#&gt; as.factor(hospital)35        0.3541110  0.0766156   4.622 3.87e-06 ***\n#&gt; as.factor(hospital)36        2.1381425  0.0773862  27.630  &lt; 2e-16 ***\n#&gt; as.factor(hospital)37        0.1404036  0.0927319   1.514 0.130049    \n#&gt; as.factor(hospital)38       -0.0868060  0.0782181  -1.110 0.267124    \n#&gt; as.factor(hospital)39       -0.0234969  0.0823390  -0.285 0.775370    \n#&gt; as.factor(hospital)40        1.1215331  0.0782181  14.339  &lt; 2e-16 ***\n#&gt; as.factor(hospital)41       -0.1497346  0.0766156  -1.954 0.050697 .  \n#&gt; as.factor(hospital)42        0.8811369  0.0850564  10.359  &lt; 2e-16 ***\n#&gt; as.factor(hospital)43       -0.7724325  0.0811676  -9.517  &lt; 2e-16 ***\n#&gt; as.factor(hospital)44        0.0344120  0.0904396   0.380 0.703588    \n#&gt; as.factor(hospital)45       -0.2137495  0.0766156  -2.790 0.005286 ** \n#&gt; as.factor(hospital)46        0.0784915  0.0823390   0.953 0.340484    \n#&gt; procedure:as.factor(month)2         NA         NA      NA       NA    \n#&gt; procedure:as.factor(month)3         NA         NA      NA       NA    \n#&gt; procedure:as.factor(month)4 -0.0750732  0.0684427  -1.097 0.272731    \n#&gt; procedure:as.factor(month)5  0.0061613  0.0684427   0.090 0.928272    \n#&gt; procedure:as.factor(month)6 -0.0531645  0.0684427  -0.777 0.437317    \n#&gt; procedure:as.factor(month)7         NA         NA      NA       NA    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.7239 on 7312 degrees of freedom\n#&gt; Multiple R-squared:  0.5334, Adjusted R-squared:  0.5299 \n#&gt; F-statistic:   152 on 55 and 7312 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\n\nWhen you use month + hospital in regression, R treats these variables as numeric, and each level gets its own coefficient. This implies a linear trend for months and a linear effect for each hospital. This might not be appropriate for months and hospitals, which are better modeled as categorical factors.\nOn the other hand, as.factor(month) + as.factor(hospital) treats both months and hospitals as factors, assigning a separate intercept for each level. This is more appropriate when dealing with categorical variables like months and hospitals because it allows for different intercepts for each level rather than assuming a linear relationship.’\nUsing as.factor(month) + as.factor(hospital) is more suitable for this analysis, as it captures the categorical nature of months and hospitals, allowing for separate effects for each level"
  },
  {
    "objectID": "content/01_journal/10_rdd.html",
    "href": "content/01_journal/10_rdd.html",
    "title": "Regression Discontinuity",
    "section": "",
    "text": "# Load required libraries\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n# Read data for the current campaign\ndf &lt;- readRDS('C:/Master Degree/Core Qualification_Compulsory Courses/Business/Causal_Data_Science/Data/Causal_Data_Science_Data/coupon.rds')\n\n\nRegression Discontinuity Design Sensitivity Analysis\n\n\n# Define cut-off\nc0 &lt;- 60\n\n# Bandwidths\nbw_orignal &lt;- c0 + c(-5,5)\nbw_half &lt;- c0 + c(-5,5) / 2\nbw_double &lt;- c0 + c(-5,5) * 2\n\nFunction to run the regression discontinuity design analysis\n\n\nrun_rdd_analysis &lt;- function(df, bw) {\n  \n  df_bw_below &lt;- df %&gt;% filter(days_since_last &gt;= bw[1] & days_since_last &lt; c0)\n  df_bw_above &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw[2])\n  df_bw &lt;- bind_rows(df_bw_above, df_bw_below)\n  \n  lm_bw &lt;- lm(purchase_after ~ days_since_last_centered + coupon, df_bw)\n  \n  \n  model_bw_below &lt;- lm(purchase_after ~ days_since_last, df_bw_below)\n  model_bw_above &lt;- lm(purchase_after ~ days_since_last, df_bw_above)\n  \n  y0 &lt;- predict(model_bw_below, tibble(days_since_last = c0))\n  y1 &lt;- predict(model_bw_above, tibble(days_since_last = c0))\n  \n  late &lt;- y1 - y0\n  return(list(LATE = late, Summary = summary(lm_bw)))\n}\n\nUse the Function for Different Bandwidths\n\n\n# Run the analysis with original bandwidth\nLATE_original &lt;- run_rdd_analysis(df, bw_orignal)\n\n# Run the analysis with half the bandwidth\nLATE_half_bandwidth &lt;- run_rdd_analysis(df, bw_half)\n\n# Run the analysis with double the bandwidth\nLATE_double_bandwidth &lt;- run_rdd_analysis(df, bw_double)\n\n\n\n\n1. Original Bandwidth\n\n\n\nThe LATE and the Summary of Using the Original Bandwidth\n\n\nCode\n# Print the results for the original bandwidth\ncat(\"LATE:\", LATE_original$LATE, \"\\n\")\n\n\n#&gt; LATE: 7.989037\n\n\nCode\nprint(LATE_original$Summary)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = df_bw)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -11.4966  -2.1312  -0.0949   2.0185  10.4159 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               11.4242     0.3965  28.813  &lt; 2e-16 ***\n#&gt; days_since_last_centered   0.3835     0.1259   3.046  0.00251 ** \n#&gt; couponTRUE                 7.9334     0.7087  11.194  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.186 on 320 degrees of freedom\n#&gt; Multiple R-squared:  0.7074, Adjusted R-squared:  0.7055 \n#&gt; F-statistic: 386.8 on 2 and 320 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n2. Half the Bandwidth\n\n\n\nThe LATE and the Summary of Using Half the Bandwidth\n\n\nCode\n# Print the results for half the bandwidth\ncat(\"LATE:\", LATE_half_bandwidth$LATE, \"\\n\")\n\n\n#&gt; LATE: 7.362377\n\n\nCode\nprint(LATE_half_bandwidth$Summary)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = df_bw)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -10.9680  -2.2013   0.1676   2.1516   8.2567 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               11.6612     0.5747  20.292  &lt; 2e-16 ***\n#&gt; days_since_last_centered   0.6883     0.3219   2.138   0.0339 *  \n#&gt; couponTRUE                 7.1679     1.0172   7.047 3.87e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.289 on 178 degrees of freedom\n#&gt; Multiple R-squared:  0.6622, Adjusted R-squared:  0.6584 \n#&gt; F-statistic: 174.5 on 2 and 178 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n3. Double the Bandwidth\n\n\n\nThe LATE and the Summary of Using Double the Bandwidth\n\n\nCode\n# Print the results for double the bandwidth\ncat(\"LATE:\", LATE_double_bandwidth$LATE, \"\\n\")\n\n\n#&gt; LATE: 9.513531\n\n\nCode\nprint(LATE_double_bandwidth$Summary)\n\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = df_bw)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -12.2718  -2.0858  -0.0003   2.0275  10.6749 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)              10.61700    0.27386  38.767   &lt;2e-16 ***\n#&gt; days_since_last_centered  0.01413    0.04255   0.332     0.74    \n#&gt; couponTRUE                9.51584    0.48628  19.569   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.115 on 626 degrees of freedom\n#&gt; Multiple R-squared:  0.7052, Adjusted R-squared:  0.7042 \n#&gt; F-statistic: 748.6 on 2 and 626 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\n\nThere is no change in the three bandwidth choices on the statistical effect (positive effect of coupon variable on purchase_after.\nThe estimated LATE is slightly lower for the half bandwidth than the original bandwidth, suggesting a more conservative estimate.\nThe estimated LATE is higher for the double bandwidth than the original bandwidth, suggesting a potentially broader impact on individuals farther from the cutoff point.\nBandwidth choice influences treatment (coupuonTRUE) effect estimation.\nCoefficients vary with bandwidth changes\n\n\n\n\n\n\nDifferent Past Campaign\n\n\n# Read data for the different past campaign\n\ndf_shipping &lt;- readRDS('C:/Master Degree/Core Qualification_Compulsory Courses/Business/Causal_Data_Science/Data/Causal_Data_Science_Data/shipping.rds')\n\n# Manipulation Testing Using Local Polynomial Density Estimation\nlibrary(rddensity) \nrddd &lt;- rddensity(df_shipping$purchase_amount, c = 30) \n\nsummary(rddd)\n\n#&gt; \n#&gt; Manipulation testing using local polynomial density estimation.\n#&gt; \n#&gt; Number of obs =       6666\n#&gt; Model =               unrestricted\n#&gt; Kernel =              triangular\n#&gt; BW method =           estimated\n#&gt; VCE method =          jackknife\n#&gt; \n#&gt; c = 30                Left of c           Right of c          \n#&gt; Number of obs         3088                3578                \n#&gt; Eff. Number of obs    2221                1955                \n#&gt; Order est. (p)        2                   2                   \n#&gt; Order bias (q)        3                   3                   \n#&gt; BW est. (h)           22.909              20.394              \n#&gt; \n#&gt; Method                T                   P &gt; |T|             \n#&gt; Robust                5.9855              0                   \n#&gt; \n#&gt; \n#&gt; P-values of binomial tests (H0: p=0.5).\n#&gt; \n#&gt; Window Length / 2          &lt;c     &gt;=c    P&gt;|T|\n#&gt; 0.261                      20      26    0.4614\n#&gt; 0.522                      41      65    0.0250\n#&gt; 0.783                      62     107    0.0007\n#&gt; 1.043                      81     136    0.0002\n#&gt; 1.304                     100     169    0.0000\n#&gt; 1.565                     114     196    0.0000\n#&gt; 1.826                     132     227    0.0000\n#&gt; 2.087                     156     263    0.0000\n#&gt; 2.348                     173     298    0.0000\n#&gt; 2.609                     191     331    0.0000\n\n\n\n\n\n\n\n\n\nThe manipulation tests indicate significant evidence of manipulation around the cut-off point (c=30).\nP-values of the robust T-statistic are close to zero, suggesting that there are systematic changes in the observed density of the purchase_amount variable near the cut-off.\nThe order of estimation and bias is also different on each side, suggesting a lack of smoothness or continuity.\nThe p-values of binomial tests provide further evidence of non-random behavior around the cut-off.\nThe purchase_amount variable, based on the results of manipulation testing, may not be appropriate as a running variable for an RDD with a cut-off at 30€.\n\n\n\n\n\n\n\nPlot to confirm that purchase_amount could not be used as a running variable at 30\n\n\n\n\nggplot(df_shipping, aes(x = purchase_amount)) + geom_histogram(binwidth = 5, fill = \"blue\", color = \"white\") + geom_vline(xintercept = 30, color = \"red\", linetype = \"dashed\") + xlab(\"Purchase Amount (€)\") + ylab(\"Number of Purchases\") + theme_minimal()"
  }
]